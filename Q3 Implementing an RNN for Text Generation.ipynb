{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz5zE3PvL2Bux2SLq29jtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19PA1AO5C4/Home-Assignment-3/blob/main/Q3%20Implementing%20an%20RNN%20for%20Text%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRo6Za-BuidG",
        "outputId": "d6b8b537-c125-4313-966a-0eaf5ffc2fdc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 97ms/step - loss: 2.5874\n",
            "Epoch 2/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 96ms/step - loss: 2.0164\n",
            "Epoch 3/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 94ms/step - loss: 1.8470\n",
            "Epoch 4/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 95ms/step - loss: 1.7427\n",
            "Epoch 5/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 95ms/step - loss: 1.6723\n",
            "Epoch 6/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 94ms/step - loss: 1.6242\n",
            "Epoch 7/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 94ms/step - loss: 1.5819\n",
            "Epoch 8/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 96ms/step - loss: 1.5462\n",
            "Epoch 9/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 95ms/step - loss: 1.5195\n",
            "Epoch 10/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 93ms/step - loss: 1.4998\n",
            "Epoch 11/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 95ms/step - loss: 1.4801\n",
            "Epoch 12/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 95ms/step - loss: 1.4615\n",
            "Epoch 13/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 97ms/step - loss: 1.4505\n",
            "Epoch 14/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 97ms/step - loss: 1.4356\n",
            "Epoch 15/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 95ms/step - loss: 1.4234\n",
            "Epoch 16/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 96ms/step - loss: 1.4130\n",
            "Epoch 17/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 95ms/step - loss: 1.4015\n",
            "Epoch 18/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 97ms/step - loss: 1.3899\n",
            "Epoch 19/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 96ms/step - loss: 1.3867\n",
            "Epoch 20/20\n",
            "\u001b[1m2905/2905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 95ms/step - loss: 1.3763\n",
            "---Generating Text with Temperature 0.2---\n",
            "--- Generating with seed: \" the thought of it!\n",
            "\n",
            "queen elizabeth:\n",
            "as\"\n",
            " the rest in the send of the strange,\n",
            "in the stranger with the great and the part,\n",
            "and the rest will not shall say it will be soul,\n",
            "the seats of the man stand to the man.\n",
            "\n",
            "king richard iii:\n",
            "so with the stander the state of the earth,\n",
            "where i say the man of the daughter of his love,\n",
            "and the people is the lady the man,\n",
            "and the people to the house of this soul.\n",
            "\n",
            "lucio:\n",
            "the greys to the bed the man of\n",
            "---Generating Text with Temperature 1.0---\n",
            "--- Generating with seed: \"\n",
            "to signify thou camest to bite the worl\"\n",
            "der\n",
            "were biglepath'd the frown we to boy,\n",
            "ho streach i resolves that i be gone,\n",
            "to say, nor paciolan murder's supposamet!\n",
            "\n",
            "lady arn:\n",
            "they weepings well, shall i will;\n",
            "add husband upon it is people?\n",
            "\n",
            "john of gaunt:\n",
            "thou't: is should he impossion sun from man.\n",
            "his lands ot plaie it would have i right\n",
            "when flire us; barn oursey's, says thereavace,\n",
            "with before sir: and det uply art god fear\n",
            "a lantanin\n",
            "---Generating Text with Temperature 1.2---\n",
            "--- Generating with seed: \"ou coward majesty! thou sleepest.\n",
            "is not\"\n",
            "h me, sicted most pidly unrawikeign\n",
            "tellsh of leasures, and with a cupporicladian!\n",
            "givuls i see foubed.\n",
            "\n",
            "benvilo:\n",
            "i small serverece of breast! oun handone.\n",
            "\n",
            "soch sengzers:\n",
            "this ost war--i should shake my suic here,\n",
            "why, will all those thas yourself the stounes hourl:\n",
            "that i'll duke all. tell the villius friaf.\n",
            "gave vinchmors, wish father!\n",
            "beace, play there with sooss! here, an,\n",
            "which old, did bano\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# 1. Load a text dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "text = text.lower()  # Lowercase for simplicity\n",
        "\n",
        "# 2. Convert text into sequences\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "maxlen = 40  # Sequence length\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# 3. Define the RNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(maxlen, len(chars))),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Train the model and generate text\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text(length, temperature):\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated + '\"')\n",
        "\n",
        "    for i in range(length):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        generated = generated[1:]\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "model.fit(x, y, batch_size=128, epochs=20) #Train for more epochs to get better results.\n",
        "\n",
        "print(\"---Generating Text with Temperature 0.2---\")\n",
        "generate_text(400, 0.2)\n",
        "\n",
        "print(\"---Generating Text with Temperature 1.0---\")\n",
        "generate_text(400, 1.0)\n",
        "\n",
        "print(\"---Generating Text with Temperature 1.2---\")\n",
        "generate_text(400, 1.2)\n",
        "\n",
        "# 5. Temperature scaling\n",
        "# Temperature scaling controls the randomness of text generation.\n",
        "# A lower temperature (e.g., 0.2) makes the model more confident in its\n",
        "# predictions, resulting in more predictable and less surprising text.\n",
        "# A higher temperature (e.g., 1.0 or 1.2) increases the randomness,\n",
        "# leading to more diverse and potentially more creative text, but also more errors.\n",
        "\n",
        "#A temperature of zero would cause the model to always chose the most likely character."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvT0Scm0uj3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}